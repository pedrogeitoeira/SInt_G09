{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent Systems (SInt) - Group Project\n",
    "\n",
    "## **Fuzzy Data Analysis with Multi-Method Classification for Predicting Heart Disease**\n",
    "\n",
    "Group No.: 9\n",
    "\n",
    "Students:\n",
    "- Pedro Geitoeira, No. 87489\n",
    "- Eloy Marquesan Dones, No. 112861\n",
    "\n",
    "**GitHub**: [SInt_G09](https://github.com/pedrogeitoeira/SInt_G09.git)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takagi-Sugeno (TS) Fuzzy Model Using **Gustafson-Kessel (GK)** Clustering\n",
    "\n",
    "**NOTE**: To ensure full reproducibility, the entire code must be run at once.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from numpy import clip, column_stack, argmax\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, cohen_kappa_score, confusion_matrix\n",
    "from pyfume.Clustering import Clusterer\n",
    "from pyfume.EstimateAntecendentSet import AntecedentEstimator\n",
    "from pyfume.EstimateConsequentParameters import ConsequentEstimator\n",
    "from pyfume.SimpfulModelBuilder import SugenoFISBuilder\n",
    "from pyfume.Tester import SugenoFISTester"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset.\n",
    "data = pd.read_csv('data/heart_disease.csv')\n",
    "\n",
    "# Extract the variable names.\n",
    "var_names = data.columns\n",
    "\n",
    "# Print the original variable names.\n",
    "print('Variable names:')\n",
    "for var in var_names:\n",
    "    print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually provide the updated variable names.\n",
    "new_var_names = ['Age',\n",
    "                 'Sex',\n",
    "                 'ChestPainType',\n",
    "                 'RestingBloodPressure',\n",
    "                 'SerumCholesterol',\n",
    "                 'FastingBloodSugar',\n",
    "                 'RestingElectrocardiographicResults',\n",
    "                 'MaximumHeartRateAchieved',\n",
    "                 'ExerciseInducedAngina',\n",
    "                 'Oldpeak',\n",
    "                 'SlopeOfThePeakExerciseSTSegment',\n",
    "                 'NumberOfMajorVessels',\n",
    "                 'Thal',\n",
    "                 'HeartDisease']\n",
    "\n",
    "# Update the column names in the dataset.\n",
    "data.columns = new_var_names\n",
    "\n",
    "# Print the updated variable names.\n",
    "print('Updated variable names:')\n",
    "for var in data.columns:\n",
    "    print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the presence of NaN values and determine if there are any non-numeric entries.\n",
    "nan_check = data.isna().sum() # Check for the presence of NaN values.\n",
    "non_numeric_check = data.apply(lambda col: col.map(lambda x: isinstance(x, str)).sum()) # Check for the presence of string entries.\n",
    "\n",
    "# Print the number of NaN entries in each variable column.\n",
    "print('Number of NaN entries in each variable column:')\n",
    "for column, count in nan_check.items():\n",
    "    print(f'{column}: {count}')\n",
    "\n",
    "# Print the number of non-numeric entries in each variable column.\n",
    "print('\\nNumber of non-numeric entries in each variable column:')\n",
    "for column, count in non_numeric_check.items():\n",
    "    print(f'{column}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The dataset appears to be clean**. It does not contain any NaN (missing) values or string (non-numeric) entries in any of the columns. Each entry in the dataset seems to be valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for class imbalance.\n",
    "class_distribution = data['HeartDisease'].value_counts(normalize = True)*100\n",
    "\n",
    "# Format the class distribution as a percentage with two decimal places.\n",
    "class_distribution = class_distribution.map('{:.2f}%'.format)\n",
    "\n",
    "# Print the class distribution.\n",
    "print('Class distribution:')\n",
    "for index, value in class_distribution.items():\n",
    "    print(f'{index}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The dataset does not exhibit significant class imbalance**. The distribution between the two classes (1 and 0) is nearly even, indicating that special techniques, such as resampling (oversampling or undersampling) or class weighting, are unlikely to be necessary for handling class imbalance during the modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the feature names.\n",
    "feature_names = data.columns[:-1]\n",
    "\n",
    "# Convert the dataset to a numpy array.\n",
    "data = data.to_numpy()\n",
    "\n",
    "# Normalize the dataset to the range [0, 1].\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Assign the X and y for the classification problem.\n",
    "X = data[:, :-1] # All columns except the last one as input features.\n",
    "y = data[:, -1] # The last column as the target (Heart Disease).\n",
    "\n",
    "# Split the dataset into training and testing sets while preserving class distribution.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The dataset was normalized to a range of 0 to 1 to ensure features are on comparable scales**. Takagi-Sugeno fuzzy models require consistent feature values to prevent distorted membership values and poor cluster formation. Clustering methods like **Gustafson-Kessel (GK)** rely on distance metrics that are sensitive to feature scales, so normalization prevents features with larger ranges from dominating, ensuring balanced fuzzy rules and accurate memberships.\n",
    "\n",
    "**The dataset was split into training and testing sets using an 80%-20% ratio**. **Stratification was applied during the split** to ensure that the class distribution was maintained across both sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the stratified k-fold cross-validation on the training set.\n",
    "k = 5 # Number of folds.\n",
    "skf = StratifiedKFold(n_splits = k, shuffle = True, random_state = 42) # 5-fold stratified cross-validation.\n",
    "\n",
    "# Define the range of cluster numbers to evaluate during training and validation.\n",
    "cluster_range = range(2, 9) # Range of cluster numbers from 2 to 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-validation** aims to assess a model’s ability to generalize to new, unseen data by splitting the dataset (in this case, the original training set) into training and validation subsets. It helps **select the best hyperparameters**, **detect overfitting**, and **provide a reliable estimate of model performance**. Specifically, **5-fold stratified cross-validation was used to maintain class distribution across each of the five folds**. This approach ensures that the model's performance metrics are representative and reduces variance and bias, leading to a more robust and fair evaluation of its ability to generalize.\n",
    "\n",
    "**To determine the optimal cluster number**, where the best estimated model performance (validation performance) is achieved, **a 5-fold stratified cross-validation was applied across a range of cluster numbers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lists to store the mean training and validation performance metrics for the different cluster numbers.\n",
    "mean_acc_train_all = [] # Mean training accuracy.\n",
    "mean_rec_train_all = [] # Mean training recall.\n",
    "mean_prec_train_all = [] # Mean training precision.\n",
    "mean_F1_score_train_all = [] # Mean training F1-score.\n",
    "mean_kappa_train_all = [] # Mean training Cohen's kappa.\n",
    "mean_acc_val_all = [] # Mean validation accuracy.\n",
    "mean_rec_val_all = [] # Mean validation recall.\n",
    "mean_prec_val_all = [] # Mean validation precision.\n",
    "mean_F1_score_val_all = [] # Mean validation F1-score.\n",
    "mean_kappa_val_all = [] # Mean validation Cohen's kappa.\n",
    "\n",
    "# Initialize the lists to store the standard deviation of the training and validation performance metrics\n",
    "# for the different cluster numbers.\n",
    "std_acc_train_all = [] # Standard deviation of the training accuracy.\n",
    "std_rec_train_all = [] # Standard deviation of the training recall.\n",
    "std_prec_train_all = [] # Standard deviation of the training precision.\n",
    "std_F1_score_train_all = [] # Standard deviation of the training F1-score.\n",
    "std_kappa_train_all = [] # Standard deviation of the training Cohen's kappa.\n",
    "std_acc_val_all = [] # Standard deviation of the validation accuracy.\n",
    "std_rec_val_all = [] # Standard deviation of the validation recall.\n",
    "std_prec_val_all = [] # Standard deviation of the validation precision.\n",
    "std_F1_score_val_all = [] # Standard deviation of the validation F1-score.\n",
    "std_kappa_val_all = [] # Standard deviation of the validation Cohen's kappa.\n",
    "\n",
    "# Loop over the different cluster numbers.\n",
    "for nr_clusters in cluster_range:\n",
    "    # Initialize the lists to store the training and validation performance metrics for each fold iteration.\n",
    "    acc_train_list = [] # Training accuracy.\n",
    "    rec_train_list = [] # Training recall.\n",
    "    prec_train_list = [] # Training precision.\n",
    "    F1_score_train_list = [] # Training F1-score.\n",
    "    kappa_train_list = [] # Training Cohen's kappa.\n",
    "    acc_val_list = [] # Validation accuracy.\n",
    "    rec_val_list = [] # Validation recall.\n",
    "    prec_val_list = [] # Validation precision.\n",
    "    F1_score_val_list = [] # Validation F1-score.\n",
    "    kappa_val_list = [] # Validation Cohen's kappa.\n",
    "\n",
    "    # Print the cluster number.\n",
    "    print(f'Number of clusters: {nr_clusters}\\n')\n",
    "\n",
    "    # Perform the stratified k-fold cross-validation on the training set.\n",
    "    for fold_itr, (train_index, val_index) in enumerate(skf.split(X_train, y_train), start = 1):\n",
    "        # Print the fold iteration.\n",
    "        print(f'Fold iteration {fold_itr}:')\n",
    "\n",
    "        # Split the training set into training and validation folds for this fold iteration.\n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        # Cluster the input-output space.\n",
    "        np.random.seed(42) # Set the random seed to ensure reproducibility for the clustering algorithm.\n",
    "        cl = Clusterer(x_train = X_train_fold, y_train = y_train_fold, nr_clus = nr_clusters)\n",
    "        clust_centers, part_matrix, _ = cl.cluster(method = 'gk') # Using the Gustafson-Kessel clustering algorithm.\n",
    "\n",
    "        # Estimate the membership functions parameters.\n",
    "        ae = AntecedentEstimator(X_train_fold, part_matrix)\n",
    "        antecedent_params = ae.determineMF()\n",
    "\n",
    "        # Estimate the consequent parameters.\n",
    "        ce = ConsequentEstimator(X_train_fold, y_train_fold, part_matrix)\n",
    "        conseq_params = ce.suglms()\n",
    "\n",
    "        # Build the first-order Takagi-Sugeno model.\n",
    "        modbuilder = SugenoFISBuilder(antecedent_params, conseq_params, feature_names, save_simpful_code = False)\n",
    "        model = modbuilder.get_model()\n",
    "\n",
    "        # Get the model predictions on the training set for this fold iteration.\n",
    "        modtester_train = SugenoFISTester(model, X_train_fold, feature_names)\n",
    "        y_train_pred_probs = clip(modtester_train.predict()[0], 0, 1)\n",
    "        y_train_pred_probs = column_stack((1 - y_train_pred_probs, y_train_pred_probs))\n",
    "        y_train_pred = argmax(y_train_pred_probs, axis = 1)\n",
    "\n",
    "        # Compute the training performance metrics for this fold iteration.\n",
    "        acc_train_list.append(accuracy_score(y_train_fold, y_train_pred))\n",
    "        rec_train_list.append(recall_score(y_train_fold, y_train_pred))\n",
    "        prec_train_list.append(precision_score(y_train_fold, y_train_pred))\n",
    "        F1_score_train_list.append(f1_score(y_train_fold, y_train_pred))\n",
    "        kappa_train_list.append(cohen_kappa_score(y_train_fold, y_train_pred))\n",
    "\n",
    "        # Get the model predictions on the validation set for this fold iteration.\n",
    "        modtester_val = SugenoFISTester(model, X_val_fold, feature_names)\n",
    "        y_val_pred_probs = clip(modtester_val.predict()[0], 0, 1)\n",
    "        y_val_pred_probs = column_stack((1 - y_val_pred_probs, y_val_pred_probs))\n",
    "        y_val_pred = argmax(y_val_pred_probs, axis = 1)\n",
    "\n",
    "        # Compute the validation performance metrics for this fold iteration.\n",
    "        acc_val_list.append(accuracy_score(y_val_fold, y_val_pred))\n",
    "        rec_val_list.append(recall_score(y_val_fold, y_val_pred))\n",
    "        prec_val_list.append(precision_score(y_val_fold, y_val_pred))\n",
    "        F1_score_val_list.append(f1_score(y_val_fold, y_val_pred))\n",
    "        kappa_val_list.append(cohen_kappa_score(y_val_fold, y_val_pred))\n",
    "\n",
    "    # Compute the mean training and validation performance metrics across all fold iterations for this cluster number.\n",
    "    mean_acc_train_all.append(np.mean(acc_train_list))\n",
    "    mean_rec_train_all.append(np.mean(rec_train_list))\n",
    "    mean_prec_train_all.append(np.mean(prec_train_list))\n",
    "    mean_F1_score_train_all.append(np.mean(F1_score_train_list))\n",
    "    mean_kappa_train_all.append(np.mean(kappa_train_list))\n",
    "    mean_acc_val_all.append(np.mean(acc_val_list))\n",
    "    mean_rec_val_all.append(np.mean(rec_val_list))\n",
    "    mean_prec_val_all.append(np.mean(prec_val_list))\n",
    "    mean_F1_score_val_all.append(np.mean(F1_score_val_list))\n",
    "    mean_kappa_val_all.append(np.mean(kappa_val_list))\n",
    "\n",
    "    # Compute the standard deviation of the training and validation performance metrics across all fold iterations\n",
    "    # for this cluster number.\n",
    "    std_acc_train_all.append(np.std(acc_train_list))\n",
    "    std_rec_train_all.append(np.std(rec_train_list))\n",
    "    std_prec_train_all.append(np.std(prec_train_list))\n",
    "    std_F1_score_train_all.append(np.std(F1_score_train_list))\n",
    "    std_kappa_train_all.append(np.std(kappa_train_list))\n",
    "    std_acc_val_all.append(np.std(acc_val_list))\n",
    "    std_rec_val_all.append(np.std(rec_val_list))\n",
    "    std_prec_val_all.append(np.std(prec_val_list))\n",
    "    std_F1_score_val_all.append(np.std(F1_score_val_list))\n",
    "    std_kappa_val_all.append(np.std(kappa_val_list))\n",
    "\n",
    "    # Print the mean and the standard deviation of the training and validation performance metrics for this cluster number.\n",
    "    print('\\nAverage performance metrics (training):')\n",
    "    print(f'- Accuracy: {mean_acc_train_all[-1]:.3f} ± {std_acc_train_all[-1]:.3f}')\n",
    "    print(f'- Recall: {mean_rec_train_all[-1]:.3f} ± {std_rec_train_all[-1]:.3f}')\n",
    "    print(f'- Precision: {mean_prec_train_all[-1]:.3f} ± {std_prec_train_all[-1]:.3f}')\n",
    "    print(f'- F1-Score: {mean_F1_score_train_all[-1]:.3f} ± {std_F1_score_train_all[-1]:.3f}')\n",
    "    print(f'- Kappa: {mean_kappa_train_all[-1]:.3f} ± {std_kappa_train_all[-1]:.3f}')\n",
    "    print('\\nAverage performance metrics (validation):')\n",
    "    print(f'- Accuracy: {mean_acc_val_all[-1]:.3f} ± {std_acc_val_all[-1]:.3f}')\n",
    "    print(f'- Recall: {mean_rec_val_all[-1]:.3f} ± {std_rec_val_all[-1]:.3f}')\n",
    "    print(f'- Precision: {mean_prec_val_all[-1]:.3f} ± {std_prec_val_all[-1]:.3f}')\n",
    "    print(f'- F1-Score: {mean_F1_score_val_all[-1]:.3f} ± {std_F1_score_val_all[-1]:.3f}')\n",
    "    print(f'- Kappa: {mean_kappa_val_all[-1]:.3f} ± {std_kappa_val_all[-1]:.3f}')\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean training and validation accuracies for the different cluster numbers.\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(cluster_range, mean_acc_train_all, label = 'Average Training Accuracy', marker = 'o')\n",
    "plt.plot(cluster_range, mean_acc_val_all, label = 'Average Validation Accuracy', marker = 'o')\n",
    "plt.xticks(cluster_range)\n",
    "plt.ylim(0.8, 0.9)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Average Accuracy vs. Number of Clusters (Without Feature Selection)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean training and validation F1-scores for the different cluster numbers.\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(cluster_range, mean_F1_score_train_all, label = 'Average Training F1-Score', marker = 'o')\n",
    "plt.plot(cluster_range, mean_F1_score_val_all, label = 'Average Validation F1-Score', marker = 'o')\n",
    "plt.xticks(cluster_range)\n",
    "plt.ylim(0.8, 0.9)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.title('Average F1-Score vs. Number of Clusters (Without Feature Selection)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the best-performing cluster number based on each validation performance metric.\n",
    "best_models_full = {}\n",
    "\n",
    "# Create a dictionary containing the mean and the standard deviation of the validation performance metrics\n",
    "# for the different cluster numbers.\n",
    "metrics_dict_full = {'Accuracy': (mean_acc_val_all, std_acc_val_all),\n",
    "                     'Recall': (mean_rec_val_all, std_rec_val_all),\n",
    "                     'Precision': (mean_prec_val_all, std_prec_val_all),\n",
    "                     'F1-Score': (mean_F1_score_val_all, std_F1_score_val_all),\n",
    "                     'Kappa': (mean_kappa_val_all, std_kappa_val_all)}\n",
    "\n",
    "# Loop through each validation performance metric to find the cluster number with the highest mean value.\n",
    "for metric_name, (mean_values, std_values) in metrics_dict_full.items():\n",
    "    best_index = np.argmax(mean_values)\n",
    "    best_nr_clusters = cluster_range[best_index]\n",
    "    best_models_full[metric_name] = best_nr_clusters\n",
    "\n",
    "# Print the best model based on each validation performance metric.\n",
    "for metric_name, best_nr_clusters in best_models_full.items():\n",
    "    best_index = cluster_range.index(best_nr_clusters)\n",
    "\n",
    "    print(f'Best model based on validation {metric_name}:')\n",
    "    print(f'- Number of clusters: {best_nr_clusters}')\n",
    "    for m_name, (mean_values, std_values) in metrics_dict_full.items():\n",
    "        print(f'- {m_name}: {mean_values[best_index]:.3f} ± {std_values[best_index]:.3f}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the number of selected features to retain.\n",
    "nr_selected_features = 9 # Number of selected features.\n",
    "\n",
    "# Initialize a dictionary to store the 9 most frequently selected features for the different cluster numbers.\n",
    "top_features_all = {}\n",
    "\n",
    "# Initialize the lists to store the mean training and validation performance metrics for the different cluster numbers.\n",
    "mean_acc_train_all = [] # Mean training accuracy.\n",
    "mean_rec_train_all = [] # Mean training recall.\n",
    "mean_prec_train_all = [] # Mean training precision.\n",
    "mean_F1_score_train_all = [] # Mean training F1-score.\n",
    "mean_kappa_train_all = [] # Mean training Cohen's kappa.\n",
    "mean_acc_val_all = [] # Mean validation accuracy.\n",
    "mean_rec_val_all = [] # Mean validation recall.\n",
    "mean_prec_val_all = [] # Mean validation precision.\n",
    "mean_F1_score_val_all = [] # Mean validation F1-score.\n",
    "mean_kappa_val_all = [] # Mean validation Cohen's kappa.\n",
    "\n",
    "# Initialize the lists to store the standard deviation of the training and validation performance metrics\n",
    "# for the different cluster numbers.\n",
    "std_acc_train_all = [] # Standard deviation of the training accuracy.\n",
    "std_rec_train_all = [] # Standard deviation of the training recall.\n",
    "std_prec_train_all = [] # Standard deviation of the training precision.\n",
    "std_F1_score_train_all = [] # Standard deviation of the training F1-score.\n",
    "std_kappa_train_all = [] # Standard deviation of the training Cohen's kappa.\n",
    "std_acc_val_all = [] # Standard deviation of the validation accuracy.\n",
    "std_rec_val_all = [] # Standard deviation of the validation recall.\n",
    "std_prec_val_all = [] # Standard deviation of the validation precision.\n",
    "std_F1_score_val_all = [] # Standard deviation of the validation F1-score.\n",
    "std_kappa_val_all = [] # Standard deviation of the validation Cohen's kappa.\n",
    "\n",
    "# Loop over the different cluster numbers.\n",
    "for nr_clusters in cluster_range:\n",
    "    # Initialize a list to store the selected features for each fold iteration.\n",
    "    selected_features_idx_list = []\n",
    "\n",
    "    # Initialize the lists to store the training and validation performance metrics for each fold iteration.\n",
    "    acc_train_list = [] # Training accuracy.\n",
    "    rec_train_list = [] # Training recall.\n",
    "    prec_train_list = [] # Training precision.\n",
    "    F1_score_train_list = [] # Training F1-score.\n",
    "    kappa_train_list = [] # Training Cohen's kappa.\n",
    "    acc_val_list = [] # Validation accuracy.\n",
    "    rec_val_list = [] # Validation recall.\n",
    "    prec_val_list = [] # Validation precision.\n",
    "    F1_score_val_list = [] # Validation F1-score.\n",
    "    kappa_val_list = [] # Validation Cohen's kappa.\n",
    "\n",
    "    # Print the cluster number.\n",
    "    print(f'Number of clusters: {nr_clusters}\\n')\n",
    "\n",
    "    # Perform the stratified k-fold cross-validation on the training set.\n",
    "    for fold_itr, (train_index, val_index) in enumerate(skf.split(X_train, y_train), start = 1):\n",
    "        # Print the fold iteration.\n",
    "        print(f'Fold iteration {fold_itr}:')\n",
    "\n",
    "        # Split the training set into training and validation folds for this fold iteration.\n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        # # Select the top 9 features based on the ANOVA F-test with the target variable.\n",
    "        # feature_selector = SelectKBest(f_classif, k = nr_selected_features)\n",
    "\n",
    "        # Select the top 9 features based on the Mutual Information (MI) with the target variable.\n",
    "        feature_selector = SelectKBest(mutual_info_classif, k = nr_selected_features)\n",
    "\n",
    "        # Fit the feature selector on the training fold and transform the data.\n",
    "        X_train_fold = feature_selector.fit_transform(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Transform the validation fold using the same selected features.\n",
    "        X_val_fold = feature_selector.transform(X_val_fold)\n",
    "\n",
    "        # Store the selected features for this fold iteration.\n",
    "        selected_feature_idx = feature_selector.get_support(indices = True).tolist()\n",
    "        selected_features_idx_list.extend(selected_feature_idx)\n",
    "\n",
    "        # Extract the selected feature names.\n",
    "        selected_feature_names = [feature_names[i] for i in selected_feature_idx]\n",
    "\n",
    "        # Cluster the input-output space.\n",
    "        np.random.seed(42) # Set the random seed to ensure reproducibility for the clustering algorithm.\n",
    "        cl = Clusterer(x_train = X_train_fold, y_train = y_train_fold, nr_clus = nr_clusters)\n",
    "        clust_centers, part_matrix, _ = cl.cluster(method = 'gk') # Using the Gustafson-Kessel clustering algorithm.\n",
    "\n",
    "        # Estimate the membership functions parameters.\n",
    "        ae = AntecedentEstimator(X_train_fold, part_matrix)\n",
    "        antecedent_params = ae.determineMF()\n",
    "\n",
    "        # Estimate the consequent parameters.\n",
    "        ce = ConsequentEstimator(X_train_fold, y_train_fold, part_matrix)\n",
    "        conseq_params = ce.suglms()\n",
    "\n",
    "        # Build the first-order Takagi-Sugeno model.\n",
    "        modbuilder = SugenoFISBuilder(antecedent_params, conseq_params, selected_feature_names,\n",
    "                                      save_simpful_code = False)\n",
    "        model = modbuilder.get_model()\n",
    "\n",
    "        # Get the model predictions on the training set for this fold iteration.\n",
    "        modtester_train = SugenoFISTester(model, X_train_fold, selected_feature_names)\n",
    "        y_train_pred_probs = clip(modtester_train.predict()[0], 0, 1)\n",
    "        y_train_pred_probs = column_stack((1 - y_train_pred_probs, y_train_pred_probs))\n",
    "        y_train_pred = argmax(y_train_pred_probs, axis = 1)\n",
    "\n",
    "        # Compute the training performance metrics for this fold iteration.\n",
    "        acc_train_list.append(accuracy_score(y_train_fold, y_train_pred))\n",
    "        rec_train_list.append(recall_score(y_train_fold, y_train_pred))\n",
    "        prec_train_list.append(precision_score(y_train_fold, y_train_pred))\n",
    "        F1_score_train_list.append(f1_score(y_train_fold, y_train_pred))\n",
    "        kappa_train_list.append(cohen_kappa_score(y_train_fold, y_train_pred))\n",
    "\n",
    "        # Get the model predictions on the validation set for this fold iteration.\n",
    "        modtester_val = SugenoFISTester(model, X_val_fold, selected_feature_names)\n",
    "        y_val_pred_probs = clip(modtester_val.predict()[0], 0, 1)\n",
    "        y_val_pred_probs = column_stack((1 - y_val_pred_probs, y_val_pred_probs))\n",
    "        y_val_pred = argmax(y_val_pred_probs, axis = 1)\n",
    "\n",
    "        # Compute the validation performance metrics for this fold iteration.\n",
    "        acc_val_list.append(accuracy_score(y_val_fold, y_val_pred))\n",
    "        rec_val_list.append(recall_score(y_val_fold, y_val_pred))\n",
    "        prec_val_list.append(precision_score(y_val_fold, y_val_pred))\n",
    "        F1_score_val_list.append(f1_score(y_val_fold, y_val_pred))\n",
    "        kappa_val_list.append(cohen_kappa_score(y_val_fold, y_val_pred))\n",
    "    \n",
    "    # Analyze the frequency of the selected features across all fold iterations for this cluster number.\n",
    "    feature_selection_frequencies = Counter(selected_features_idx_list)\n",
    "\n",
    "    # Extract and store the 9 most frequently selected features for this cluster number.\n",
    "    top_features = [feature for feature, _ in feature_selection_frequencies.most_common(nr_selected_features)]\n",
    "    top_features_all[nr_clusters] = sorted(top_features) # Store them in the dictionary.\n",
    "\n",
    "    # Compute the mean training and validation performance metrics across all fold iterations for this cluster number.\n",
    "    mean_acc_train_all.append(np.mean(acc_train_list))\n",
    "    mean_rec_train_all.append(np.mean(rec_train_list))\n",
    "    mean_prec_train_all.append(np.mean(prec_train_list))\n",
    "    mean_F1_score_train_all.append(np.mean(F1_score_train_list))\n",
    "    mean_kappa_train_all.append(np.mean(kappa_train_list))\n",
    "    mean_acc_val_all.append(np.mean(acc_val_list))\n",
    "    mean_rec_val_all.append(np.mean(rec_val_list))\n",
    "    mean_prec_val_all.append(np.mean(prec_val_list))\n",
    "    mean_F1_score_val_all.append(np.mean(F1_score_val_list))\n",
    "    mean_kappa_val_all.append(np.mean(kappa_val_list))\n",
    "\n",
    "    # Compute the standard deviation of the training and validation performance metrics across all fold iterations\n",
    "    # for this cluster number.\n",
    "    std_acc_train_all.append(np.std(acc_train_list))\n",
    "    std_rec_train_all.append(np.std(rec_train_list))\n",
    "    std_prec_train_all.append(np.std(prec_train_list))\n",
    "    std_F1_score_train_all.append(np.std(F1_score_train_list))\n",
    "    std_kappa_train_all.append(np.std(kappa_train_list))\n",
    "    std_acc_val_all.append(np.std(acc_val_list))\n",
    "    std_rec_val_all.append(np.std(rec_val_list))\n",
    "    std_prec_val_all.append(np.std(prec_val_list))\n",
    "    std_F1_score_val_all.append(np.std(F1_score_val_list))\n",
    "    std_kappa_val_all.append(np.std(kappa_val_list))\n",
    "\n",
    "    # Print the names of the 9 most frequently selected features for this cluster number.\n",
    "    top_feature_names = [feature_names[i] for i in sorted(top_features)]\n",
    "    print(f'\\n{nr_selected_features} most frequently selected features:')\n",
    "    for feature in top_feature_names:\n",
    "        print(f'- {feature}')\n",
    "\n",
    "    # Print the mean and the standard deviation of the training and validation performance metrics for this cluster number.\n",
    "    print('\\nAverage performance metrics (training):')\n",
    "    print(f'- Accuracy: {mean_acc_train_all[-1]:.3f} ± {std_acc_train_all[-1]:.3f}')\n",
    "    print(f'- Recall: {mean_rec_train_all[-1]:.3f} ± {std_rec_train_all[-1]:.3f}')\n",
    "    print(f'- Precision: {mean_prec_train_all[-1]:.3f} ± {std_prec_train_all[-1]:.3f}')\n",
    "    print(f'- F1-Score: {mean_F1_score_train_all[-1]:.3f} ± {std_F1_score_train_all[-1]:.3f}')\n",
    "    print(f'- Kappa: {mean_kappa_train_all[-1]:.3f} ± {std_kappa_train_all[-1]:.3f}')\n",
    "    print('\\nAverage performance metrics (validation):')\n",
    "    print(f'- Accuracy: {mean_acc_val_all[-1]:.3f} ± {std_acc_val_all[-1]:.3f}')\n",
    "    print(f'- Recall: {mean_rec_val_all[-1]:.3f} ± {std_rec_val_all[-1]:.3f}')\n",
    "    print(f'- Precision: {mean_prec_val_all[-1]:.3f} ± {std_prec_val_all[-1]:.3f}')\n",
    "    print(f'- F1-Score: {mean_F1_score_val_all[-1]:.3f} ± {std_F1_score_val_all[-1]:.3f}')\n",
    "    print(f'- Kappa: {mean_kappa_val_all[-1]:.3f} ± {std_kappa_val_all[-1]:.3f}')\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature selection was performed during cross-validation, specifically on the training set of each fold iteration**. This prevents the model from indirectly learning from the validation set, avoids data leakage, and provides an unbiased estimate of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean training and validation accuracies for the different cluster numbers.\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(cluster_range, mean_acc_train_all, label = 'Average Training Accuracy', marker = 'o')\n",
    "plt.plot(cluster_range, mean_acc_val_all, label = 'Average Validation Accuracy', marker = 'o')\n",
    "plt.xticks(cluster_range)\n",
    "plt.ylim(0.80, 0.90)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Average Accuracy vs. Number of Clusters (With Feature Selection)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean training and validation F1-scores for the different cluster numbers.\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(cluster_range, mean_F1_score_train_all, label = 'Average Training F1-Score', marker = 'o')\n",
    "plt.plot(cluster_range, mean_F1_score_val_all, label = 'Average Validation F1-Score', marker = 'o')\n",
    "plt.xticks(cluster_range)\n",
    "plt.ylim(0.80, 0.90)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.title('Average F1-Score vs. Number of Clusters (With Feature Selection)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the best-performing cluster number based on each validation performance metric.\n",
    "best_models_fs = {}\n",
    "\n",
    "# Create a dictionary containing the mean and the standard deviation of the validation performance metrics\n",
    "# for the different cluster numbers.\n",
    "metrics_dict_fs = {'Accuracy': (mean_acc_val_all, std_acc_val_all),\n",
    "                   'Recall': (mean_rec_val_all, std_rec_val_all),\n",
    "                   'Precision': (mean_prec_val_all, std_prec_val_all),\n",
    "                   'F1-Score': (mean_F1_score_val_all, std_F1_score_val_all),\n",
    "                   'Kappa': (mean_kappa_val_all, std_kappa_val_all)}\n",
    "\n",
    "# Loop through each validation performance metric to find the cluster number with the highest mean value.\n",
    "for metric_name, (mean_values, std_values) in metrics_dict_fs.items():\n",
    "    best_index = np.argmax(mean_values)\n",
    "    best_nr_clusters = cluster_range[best_index]\n",
    "    best_models_fs[metric_name] = best_nr_clusters\n",
    "\n",
    "# Print the best model based on each validation performance metric.\n",
    "for metric_name, best_nr_clusters in best_models_fs.items():\n",
    "    best_index = cluster_range.index(best_nr_clusters)\n",
    "\n",
    "    print(f'Best model based on validation {metric_name}:')\n",
    "    print(f'- Number of clusters: {best_nr_clusters}')\n",
    "    for m_name, (mean_values, std_values) in metrics_dict_fs.items():\n",
    "        print(f'- {m_name}: {mean_values[best_index]:.3f} ± {std_values[best_index]:.3f}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting and Training the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the best model between the full feature set and the selected feature set\n",
    "# based on each validation performance metric.\n",
    "best_models = {}\n",
    "\n",
    "# Loop through each validation performance metric to compare the best models from the full feature set\n",
    "# and the selected feature set based on the mean values.\n",
    "for metric_name in metrics_dict_fs.keys():\n",
    "    # Extract the best cluster number in both the full feature set and the selected feature set scenarios\n",
    "    # for this validation performance metric.\n",
    "    full_nr_clusters = best_models_full[metric_name]\n",
    "    fs_nr_clusters = best_models_fs[metric_name]\n",
    "\n",
    "    # Extract the means of the validation performance metrics for the best cluster number in both the full feature set\n",
    "    # and the selected feature set scenarios for this validation performance metric.\n",
    "    full_mean = metrics_dict_full[metric_name][0][cluster_range.index(full_nr_clusters)]\n",
    "    fs_mean = metrics_dict_fs[metric_name][0][cluster_range.index(fs_nr_clusters)]\n",
    "\n",
    "    # Choose the best model based on the highest mean value for this validation performance metric; in case\n",
    "    # of a tie, use the lowest standard deviation.\n",
    "    if full_mean > fs_mean:\n",
    "        best_models[metric_name] = ('Full Feature Set', full_nr_clusters)\n",
    "    elif fs_mean > full_mean:\n",
    "        best_models[metric_name] = (f'{nr_selected_features} Features (Feature Selection)', fs_nr_clusters)\n",
    "    else:\n",
    "        # If the mean values are equal, choose the model with the lower standard deviation.\n",
    "        full_std = metrics_dict_full[metric_name][1][cluster_range.index(full_nr_clusters)]\n",
    "        fs_std = metrics_dict_fs[metric_name][1][cluster_range.index(fs_nr_clusters)]\n",
    "        if full_std < fs_std:\n",
    "            best_models[metric_name] = ('Full Feature Set', full_nr_clusters)\n",
    "        else:\n",
    "            best_models[metric_name] = (f'{nr_selected_features} Features (Feature Selection)', fs_nr_clusters)\n",
    "\n",
    "# Print the final best model based on each validation performance metric.\n",
    "for metric_name, (feature_set_type, best_nr_clusters) in best_models.items():\n",
    "    best_index = cluster_range.index(best_nr_clusters)\n",
    "    if feature_set_type == 'Full Feature Set':\n",
    "        metrics_dict = metrics_dict_full\n",
    "    else:\n",
    "        metrics_dict = metrics_dict_fs\n",
    "\n",
    "    print(f'Best model based on validation {metric_name}:')\n",
    "    print(f'- Feature set: {feature_set_type}')\n",
    "    print(f'- Number of clusters: {best_nr_clusters}')\n",
    "    for m_name, (mean_values, std_values) in metrics_dict.items():\n",
    "        print(f'- {m_name}: {mean_values[best_index]:.3f} ± {std_values[best_index]:.3f}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the final best model based on validation F1-Score.\n",
    "decision_metric_name = 'F1-Score' # Metrics by priority order: 'F1-Score', Accuracy', 'Recall', 'Precision'.\n",
    "f1_best_model = best_models[decision_metric_name]\n",
    "\n",
    "# Extract the details of the final best model based on validation F1-Score.\n",
    "final_feature_set_type, final_nr_clusters = f1_best_model\n",
    "\n",
    "\n",
    "# Print the final best model based on validation F1-Score.\n",
    "\n",
    "best_index = cluster_range.index(final_nr_clusters)\n",
    "if final_feature_set_type == 'Full Feature Set':\n",
    "    metrics_dict = metrics_dict_full\n",
    "else:\n",
    "    metrics_dict = metrics_dict_fs\n",
    "\n",
    "print(f'Best model based on validation {decision_metric_name}:')\n",
    "print(f'- Feature set: {final_feature_set_type}')\n",
    "print(f'- Number of clusters: {final_nr_clusters}')\n",
    "for m_name, (mean_values, std_values) in metrics_dict.items():\n",
    "    print(f'- {m_name}: {mean_values[best_index]:.3f} ± {std_values[best_index]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the input-output space using the entire training set and the cluster number\n",
    "# from the final best model based on validation performance.\n",
    "np.random.seed(42) # Set the random seed to ensure reproducibility for the clustering algorithm.\n",
    "cl = Clusterer(x_train = X_train, y_train = y_train, nr_clus = final_nr_clusters)\n",
    "clust_centers, part_matrix, _ = cl.cluster(method = 'gk') # Using the Gustafson-Kessel clustering algorithm.\n",
    "\n",
    "if final_feature_set_type == 'Full Feature Set':\n",
    "    # Use all the features.\n",
    "    final_feature_names = feature_names\n",
    "    X_train_final = X_train\n",
    "    X_test_final = X_test\n",
    "else:\n",
    "    # # Select the top 9 features based on the ANOVA F-test with the target variable.\n",
    "    # feature_selector = SelectKBest(f_classif, k = nr_selected_features)\n",
    "\n",
    "    # Select the top 9 features based on the Mutual Information (MI) with the target variable.\n",
    "    feature_selector = SelectKBest(mutual_info_classif, k = nr_selected_features)\n",
    "\n",
    "    # Fit the feature selector on the original training set and transform the data.\n",
    "    X_train_final = feature_selector.fit_transform(X_train, y_train)\n",
    "\n",
    "    # Transform the testing set using the same selected features.\n",
    "    X_test_final = feature_selector.transform(X_test)\n",
    "\n",
    "    # Extract the selected feature names.\n",
    "    final_feature_idx = feature_selector.get_support(indices = True).tolist()\n",
    "    final_feature_names = [feature_names[i] for i in final_feature_idx]\n",
    "\n",
    "# Estimate the membership functions parameters.\n",
    "ae = AntecedentEstimator(X_train_final, part_matrix)\n",
    "antecedent_params = ae.determineMF()\n",
    "\n",
    "# Estimate the consequent parameters.\n",
    "ce = ConsequentEstimator(X_train_final, y_train, part_matrix)\n",
    "conseq_params = ce.suglms()\n",
    "\n",
    "# Build the first-order Takagi-Sugeno model.\n",
    "modbuilder = SugenoFISBuilder(antecedent_params, conseq_params, final_feature_names, save_simpful_code = False)\n",
    "model = modbuilder.get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The optimal number of clusters and feature set type for the final model were selected based on the validation performance**. The following metrics, in priority order, were taken into account for this selection:\n",
    "1. **F1-Score**: Balances Recall and Precision, providing an overall measure of the model's ability to handle both false positives and false negatives effectively, making it the **decision metric** for the final model selection.\n",
    "2. **Accuracy**: Given that the dataset is balanced, Accuracy is a useful metric as it gives a good overall sense of how well the model is performing across both classes (heart disease and no heart disease). A high Accuracy indicates that the model is making correct predictions most of the time.\n",
    "3. **Recall**: In a medical diagnosis context like heart disease, having high Recall ensures that fewer cases of actual heart disease go undiagnosed (minimizing false negatives), which is crucial for patient safety.\n",
    "4. **Precision**: If there are significant costs associated with false positives, such as unnecessary stress tests or invasive procedures, Precision should also be taken into account to ensure positive predictions are as accurate as possible.\n",
    "Given that F1-Score is the metric with more priority, it end up being chosen as the decision metric.\n",
    "\n",
    "**The final model was then trained on the entire training set**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the final model predictions on the testing set.\n",
    "modtester = SugenoFISTester(model, X_test_final, final_feature_names)\n",
    "y_pred_probs = clip(modtester.predict()[0], 0, 1)\n",
    "y_pred_probs = column_stack((1 - y_pred_probs, y_pred_probs))\n",
    "y_pred = argmax(y_pred_probs, axis = 1)\n",
    "\n",
    "# Compute the test perfomance metrics for the final model.\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "F1_score = f1_score(y_test, y_pred)\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "# Print the test performance metrics for the final model.\n",
    "print('Test performance metrics for the final model:')\n",
    "print(f'- Accuracy: {acc:.3f}')\n",
    "print(f'- Recall: {rec:.3f}')\n",
    "print(f'- Precision: {prec:.3f}')\n",
    "print(f'- F1-Score: {F1_score:.3f}')\n",
    "print(f'- Kappa Score: {kappa:.3f}')\n",
    "\n",
    "# Compute the confusion matrix for the final model.\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the confusion matrix for the final model.\n",
    "print(f'\\nConfusion matrix for the final model:\\n{cm}')\n",
    "\n",
    "# Plot the confusion matrix for the final model.\n",
    "plt.figure(figsize = (8, 6))\n",
    "sns.heatmap(cm, annot = True, fmt = 'd', cmap = 'coolwarm',\n",
    "            xticklabels = ['Predicted Negative', 'Predicted Positive'],\n",
    "            yticklabels = ['Actual Negative', 'Actual Positive'])\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xlabel('Predicted Labels', fontsize = 14)\n",
    "plt.ylabel('Actual Labels', fontsize = 14)\n",
    "plt.title('Confusion Matrix for the Final Model', fontsize = 16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intelligent-systems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
